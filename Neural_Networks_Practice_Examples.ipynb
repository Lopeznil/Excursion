{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdTi8dbk21VRPITh4oPIQY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lopeznil/Excursion/blob/main/Neural_Networks_Practice_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDcg5fCg3g_b"
      },
      "outputs": [],
      "source": [
        "#IMAGE CLASSIFIER\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "print(\"\\nLoading training data...\")\n",
        "\n",
        "training_data_generator = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05)\n",
        "\n",
        "training_iterator = training_data_generator.flow_from_directory('data/train',class_mode='categorical',color_mode='grayscale',batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(\"\\nLoading validation data...\")\n",
        "\n",
        "#1) Create validation_data_generator, an ImageDataGenerator that just performs pixel normalization:\n",
        "\n",
        "validation_data_generator = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "#2) Use validation_data_generator.flow_from_directory(...) to load the validation data from the 'data/test' folder:\n",
        "\n",
        "validation_iterator = validation_data_generator.flow_from_directory('data/test', class_mode='categorical', color_mode='grayscale', batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(\"\\nBuilding model...\")\n",
        "\n",
        "#Rebuilds our model from the previous exercise, with convolutional and max pooling layers:\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(256, 256, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(2, 5, strides=3, activation=\"relu\"))\n",
        "#model.add(tf.keras.layers.Conv2D(2, 5, strides=3, padding='valid', activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D(\n",
        "    pool_size=(5, 5), strides=(5,5)))\n",
        "model.add(tf.keras.layers.Conv2D(4, 3, strides=1, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPooling2D(\n",
        "    pool_size=(2,2), strides=(2,2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(2,activation=\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "print(\"\\nCompiling model...\")\n",
        "\n",
        "#3) Compile the model with an Adam optimizer, Categorical Cross Entropy Loss, and Accuracy and AUC metrics:\n",
        "\n",
        "model.compile(\n",
        "   optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "   loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "   metrics=[tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.AUC()]\n",
        ")\n",
        "\n",
        "print(\"\\nTraining model...\")\n",
        "\n",
        "#4) Use model.fit(...) to train and validate our model for 5 epochs:\n",
        "\n",
        "model.fit(\n",
        "       training_iterator,\n",
        "       steps_per_epoch=training_iterator.samples/BATCH_SIZE,\n",
        "       epochs=5,\n",
        "       validation_data=validation_iterator,\n",
        "       validation_steps=validation_iterator.samples/BATCH_SIZE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#REGESSOR\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "tensorflow.random.set_seed(35) #for the reproducibility of results\n",
        "\n",
        "def design_model(features):\n",
        "  model = Sequential(name = \"my_first_model\")\n",
        "  #without hard-coding\n",
        "  input = InputLayer(input_shape=(features.shape[1],))\n",
        "  #add the input layer\n",
        "  model.add(input)\n",
        "  #add a hidden layer with 128 neurons\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #add an output layer to our model\n",
        "  model.add(Dense(1))\n",
        "  opt = Adam(learning_rate=0.1)\n",
        "  model.compile(loss='mse',  metrics=['mae'], optimizer=opt)\n",
        "  return model\n",
        "\n",
        "dataset = pd.read_csv('insurance.csv') #load the dataset\n",
        "features = dataset.iloc[:,0:6] #choose first 7 columns as features\n",
        "labels = dataset.iloc[:,-1] #choose the final column for prediction\n",
        "\n",
        "features = pd.get_dummies(features) #one-hot encoding for categorical variables\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) #split the data into training and test data\n",
        "\n",
        "#standardize\n",
        "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "features_train = ct.fit_transform(features_train)\n",
        "features_test = ct.transform(features_test)\n",
        "\n",
        "#invoke the function for our model design\n",
        "model = design_model(features_train)\n",
        "print(model.summary())\n",
        "\n",
        "#fit the model using 40 epochs and batch size 1\n",
        "model.fit(features_train, labels_train, epochs=40, batch_size=1, verbose=1)\n",
        "\n",
        "#evaluate the model on the test data\n",
        "val_mse, val_mae = model.evaluate(features_test, labels_test, verbose = 0)\n",
        "\n",
        "print(\"MAE: \", val_mae)\n"
      ],
      "metadata": {
        "id": "jgeyolQU5A6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "Nsybl1UABhM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson, you learned how to both manually and automatically choose hyperparameters of the neural network training procedure in order to select a model with the best predictive performance on a validation set. The hyperparameters we covered in this lesson are\n",
        "\n",
        "    learning rate\n",
        "    batch size\n",
        "    number of epochs\n",
        "    model size (number of hidden layers/neurons and number of parameters)\n",
        "    regularization (dropout)\n",
        "\n",
        "We discussed the concepts of underfitting (having a too simple model to capture data patterns) and overfitting (having a model with too many parameters that learns the training data too well and is unable to generalize). We discussed methods to combat overfitting such as regularization. To avoid underfitting we increased the complexity of our model.\n",
        "\n",
        "Besides data preprocessing, hyperparameter tuning is probably the most costly and intensive process of neural network training. We covered how to set up grid seach and randomized search in Keras in order to automate the process of hyperparameter tuning.\n",
        "\n",
        "We also showed you how to check the performance of your model against a simple baseline. Baselines give you an idea of whether your model has a reasonable performance"
      ],
      "metadata": {
        "id": "MIVIiO5oAMmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the batch contains all the training examples, the process is called batch gradient descent. If the batch has one sample, it is called the stochastic gradient descent. And finally, when 1 < batch size < number of training points, is called mini-batch gradient descent. An advantage of using batches is for GPU computation that can parallelize neural network computations.\n",
        "\n",
        "How do we choose the batch size for our model? On one hand, a larger batch size will provide our model with better gradient estimates and a solution close to the optimum, but this comes at a cost of computational efficiency and good generalization performance. On the other hand, smaller batch size is a poor estimate of the gradient, but the learning is performed faster. Finding the “sweet spot” depends on the dataset and the problem, and can be determined through hyperparameter tuning."
      ],
      "metadata": {
        "id": "IfrVz_M-D2NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TUNING WITH HYPERPARAMATERS\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "tf.random.set_seed(42) #for reproducibility of result we always use the same seed for random number generator\n",
        "\n",
        "dataset = pd.read_csv(\"insurance.csv\") #read the dataset\n",
        "\n",
        "def fit_model(model, f_train, l_train, learning_rate, num_epochs):\n",
        "    #train the model on the training data\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 20)\n",
        "    history = model.fit(features_train, labels_train, epochs=num_epochs, batch_size= 16, verbose=0, validation_split = 0.2, callbacks = [es])\n",
        "    return history\n",
        "\n",
        "features = dataset.iloc[:,0:6] #choose first 7 columns as features\n",
        "labels = dataset.iloc[:,-1] #choose the final column for prediction\n",
        "\n",
        "features = pd.get_dummies(features) #one hot encoding for categorical variables\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "#standardize\n",
        "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
        "features_train = ct.fit_transform(features_train) #gives numpy arrays\n",
        "features_test = ct.transform(features_test) #gives numpy arrays\n",
        "\n",
        "#______________________________________________________________________________________________________________________________________________________________________\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from plotting import plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def design_model_dropout(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = tf.keras.Input(shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(24, activation='relu'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    #------your code here!------\n",
        "\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "\n",
        "def design_model_no_dropout(X, learning_rate):\n",
        "    model = Sequential(name=\"my_first_model\")\n",
        "    input = layers.InputLayer(input_shape=(X.shape[1],))\n",
        "    model.add(input)\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(24, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
        "    return model\n",
        "\n",
        "#using the early stopping in fit_model\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "#train the model without dropout\n",
        "history1 = fit_model(design_model_no_dropout(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "#train the model with dropout\n",
        "history2 = fit_model(design_model_dropout(features_train, learning_rate), features_train, labels_train, learning_rate, num_epochs)\n",
        "\n",
        "plot(history1, 'static/images/no_dropout.png')\n",
        "\n",
        "plot(history2, 'static/images/with_dropout.png')\n",
        "\n",
        "import app #don't worry about this. This is to show you the plot in the browser.\n",
        "\n"
      ],
      "metadata": {
        "id": "4SSrPTDS9KYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need a baseline? For example, we have data consisting of 90% dog images, and 10% cat images. An algorithm that predicts the majority class for each data point, will have 90% accuracy on this dataset! That might sound good, but predicting the majority class is hardly a useful classifier. We need to perform better.\n",
        "\n",
        "A baseline result is the simplest possible prediction. For some problems, this may be a random result, and for others, it may be the most common class prediction. Since we are focused on a regression task in this lesson, we can use averages or medians of the class distribution known as central tendency measures as the result for all predictions.\n",
        "\n",
        "Scikit-learn provides DummyRegressor, which serves as a baseline regression algorithm. We’ll choose mean (average) as our central tendency measure"
      ],
      "metadata": {
        "id": "YH379RQ4ACja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "dummy_regr = DummyRegressor(strategy=\"median\")\n",
        "dummy_regr.fit(features_train, labels_train)\n",
        "y_pred = dummy_regr.predict(features_test)\n",
        "MAE_baseline = mean_absolute_error(labels_test, y_pred)\n",
        "print(MAE_baseline)"
      ],
      "metadata": {
        "id": "Je62AQ9n_5GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we’ve been manually setting and adjusting hyperparameters to train and evaluate our model. If we didn’t like the result, we changed the hyperparameters to some other values. However, this is rather cumbersome; it would be nice if we could make these changes in a systematic and automated way. Fortunately, there are some strategies for automated hyperparameter tuning, including the following two.\n",
        "\n",
        "Grid search, or exhaustive search, tries every combination of desired hyperparameter values. If, for example, we want to try learning rates of 0.01 and 0.001 and batch sizes of 10, 30, and 50, grid search will try six combinations of parameters (0.01 and 10, 0.01 and 30, 0.01 and 50, 0.001 and 10, and so on). This obviously gets very computationally demanding when we increase the number of values per hyperparameter or the number of hyperparameters we want to tune.\n",
        "\n",
        "On the other hand, Random Search goes through random combinations of hyperparameters and doesn’t try them all.\n",
        "\n",
        "Grid search in Keras\n",
        "\n",
        "To use GridSearchCV from scikit-learn for regression we need to first wrap our neural network model into a\n",
        "\n",
        "    KerasRegressor:\n",
        "\n",
        "    model = KerasRegressor(build_fn=design_model)\n",
        "\n",
        "Then we need to setup the desired hyperparameters grid (we don’t use many values for the sake of speed):\n",
        "\n",
        "    batch_size = [10, 40]\n",
        "    epochs = [10, 50]\n",
        "    param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "\n",
        "\n",
        "Finally, we initialize a GridSearchCV object and fit our model to the data:\n",
        "\n",
        "    grid = GridSearchCV(estimator = model, param_grid=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False))\n",
        "    \n",
        "    grid_result = grid.fit(features_train, labels_train, verbose = 0)\n",
        "\n",
        "\n",
        "\n",
        "Notice that we initialized the scoring parameter with scikit-learn’s .make_scorer() method. We’re evaluating our hyperparameter combinations with a mean squared error making sure that greater_is_better is set to False since we are searching for a set of hyperparameters that yield us the smallest error.\n",
        "\n",
        "Randomized search in Keras\n",
        "\n",
        "We first change our hyperparameter grid specification for the randomized search in order to have more options:\n",
        "\n",
        "    param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(10, 100)}\n",
        "\n",
        "\n",
        "\n",
        "Randomized search will sample values for batch_size and nb_epoch from uniform distributions in the interval [2, 16] and [10, 100], respectively, for a fixed number of iterations. In our case, 12 iterations:\n",
        "\n",
        "    grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False), n_iter = 12)\n",
        "\n",
        "\n",
        "\n",
        "We cover only simpler cases here, but you can set up GridSearchCV and RandomizedSearchCV to tune over any hyperparameters you can think of: optimizers, number of hidden layers, number of neurons per layer, and so on."
      ],
      "metadata": {
        "id": "FewPqEr8AppG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import make_scorer\n",
        "from model import design_model, features_train, labels_train\n",
        "\n",
        "#------------- GRID SEARCH --------------\n",
        "# def do_grid_search():\n",
        "#   batch_size = [6, 64]\n",
        "#   epochs = [10, 50]\n",
        "#   model = KerasRegressor(build_fn=design_model)\n",
        "#   param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "#   grid = GridSearchCV(estimator = model, param_grid=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False),return_train_score = True)\n",
        "#   grid_result = grid.fit(features_train, labels_train, verbose = 0)\n",
        "#   print(grid_result)\n",
        "#   print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "#   means = grid_result.cv_results_['mean_test_score']\n",
        "#   stds = grid_result.cv_results_['std_test_score']\n",
        "#   params = grid_result.cv_results_['params']\n",
        "#   for mean, stdev, param in zip(means, stds, params):\n",
        "#       print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "#   print(\"Traininig\")\n",
        "#   means = grid_result.cv_results_['mean_train_score']\n",
        "#   stds = grid_result.cv_results_['std_train_score']\n",
        "#   for mean, stdev, param in zip(means, stds, params):\n",
        "#       print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "#------------- RANDOMIZED SEARCH --------------\n",
        "def do_randomized_search():\n",
        "  param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(10, 100)}\n",
        "  model = KerasRegressor(build_fn=design_model)\n",
        "  grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False), n_iter = 12)\n",
        "  grid_result = grid.fit(features_train, labels_train, verbose = 0)\n",
        "  print(grid_result)\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "# print(\"-------------- GRID SEARCH --------------------\")\n",
        "# do_grid_search()\n",
        "print(\"-------------- RANDOMIZED SEARCH --------------------\")\n",
        "do_randomized_search()\n"
      ],
      "metadata": {
        "id": "NY0QjlijAfib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4LsUYxxD13q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}